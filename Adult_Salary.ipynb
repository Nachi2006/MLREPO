{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nachi2006/MLREPO/blob/main/Adult_Salary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CODECHEF-VIT RECRUITMENTS 2025"
      ],
      "metadata": {
        "id": "WqM-1AizB0Mk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnJJZkt89jBI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\",\n",
        "           \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
        "data = pd.read_csv(url, names=columns, skipinitialspace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_curve, auc\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "PexzL3oi_Pox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166ff0fa-5073-4087-9699-6b63e52f6ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl (223.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.25.1 xgboost-2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAnalyzer:\n",
        "    def __init__(self, data):\n",
        "        \"\"\"Initialize with a pandas DataFrame\"\"\"\n",
        "        self.data = data\n",
        "        self.numerical_features = None\n",
        "        self.categorical_features = None\n",
        "        self.target = None\n",
        "\n",
        "    def analyze_data_quality(self):\n",
        "        \"\"\"Check data quality: missing values, duplicates, and data types\"\"\"\n",
        "        print(\"=== Data Quality Analysis ===\")\n",
        "        print(\"\\nDataset Shape:\", self.data.shape)\n",
        "        print(\"\\nData Types:\\n\", self.data.dtypes)\n",
        "        print(\"\\nMissing Values:\\n\", self.data.isnull().sum())\n",
        "        print(\"\\nDuplicate Rows:\", self.data.duplicated().sum())\n",
        "\n",
        "        # Memory usage\n",
        "        memory_usage = self.data.memory_usage(deep=True).sum() / 1024**2\n",
        "        print(f\"\\nMemory Usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"Clean the dataset\"\"\"\n",
        "        print(\"\\n=== Data Cleaning ===\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        initial_rows = len(self.data)\n",
        "        self.data.drop_duplicates(inplace=True)\n",
        "        print(f\"Removed {initial_rows - len(self.data)} duplicate rows\")\n",
        "\n",
        "        # Handle missing values\n",
        "        for column in self.data.columns:\n",
        "            if self.data[column].isnull().sum() > 0:\n",
        "                if self.data[column].dtype in ['int64', 'float64']:\n",
        "                    # Fill numerical missing values with median\n",
        "                    self.data[column].fillna(self.data[column].median(), inplace=True)\n",
        "                else:\n",
        "                    # Fill categorical missing values with mode\n",
        "                    self.data[column].fillna(self.data[column].mode()[0], inplace=True)\n",
        "\n",
        "        print(\"Missing values handled\")\n",
        "\n",
        "    def perform_eda(self):\n",
        "        \"\"\"Perform Exploratory Data Analysis\"\"\"\n",
        "        print(\"\\n=== Exploratory Data Analysis ===\")\n",
        "\n",
        "        # Separate numerical and categorical columns\n",
        "        self.numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
        "        self.categorical_features = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Descriptive statistics\n",
        "        print(\"\\nNumerical Features Summary:\")\n",
        "        print(self.data[self.numerical_features].describe())\n",
        "\n",
        "        print(\"\\nCategorical Features Summary:\")\n",
        "        for cat_col in self.categorical_features:\n",
        "            print(f\"\\n{cat_col} value counts:\")\n",
        "            print(self.data[cat_col].value_counts().head())\n",
        "\n",
        "        # Create visualizations directory\n",
        "        import os\n",
        "        os.makedirs('visualizations', exist_ok=True)\n",
        "\n",
        "        # Distribution plots for numerical features\n",
        "        self._plot_distributions()\n",
        "\n",
        "        # Correlation analysis\n",
        "        self._plot_correlation()\n",
        "\n",
        "        # Categorical feature analysis\n",
        "        self._analyze_categorical_features()\n",
        "\n",
        "        if 'date' in self.data.columns:\n",
        "            self._analyze_time_trends()\n",
        "\n",
        "    def _plot_distributions(self):\n",
        "        \"\"\"Plot distributions for numerical features\"\"\"\n",
        "        for col in self.numerical_features:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.histplot(self.data[col], kde=True)\n",
        "            plt.title(f'Distribution of {col}')\n",
        "            plt.savefig(f'visualizations/{col}_distribution.png')\n",
        "            plt.close()\n",
        "\n",
        "    def _plot_correlation(self):\n",
        "        \"\"\"Plot correlation heatmap\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(self.data[self.numerical_features].corr(),\n",
        "                   annot=True, cmap='coolwarm', center=0)\n",
        "        plt.title('Correlation Heatmap')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('visualizations/correlation_heatmap.png')\n",
        "        plt.close()\n",
        "\n",
        "    def _analyze_categorical_features(self):\n",
        "        \"\"\"Analyze categorical features\"\"\"\n",
        "        for col in self.categorical_features:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            self.data[col].value_counts().plot(kind='bar')\n",
        "            plt.title(f'Distribution of {col}')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'visualizations/{col}_distribution.png')\n",
        "            plt.close()\n",
        "\n",
        "    def _analyze_time_trends(self):\n",
        "        \"\"\"Analyze time-based trends if date column exists\"\"\"\n",
        "        if 'date' in self.data.columns:\n",
        "            self.data['date'] = pd.to_datetime(self.data['date'])\n",
        "            self.data['month'] = self.data['date'].dt.month\n",
        "            self.data['day_of_week'] = self.data['date'].dt.dayofweek\n",
        "\n",
        "            # Monthly trends\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            monthly_data = self.data.groupby('month').size()\n",
        "            monthly_data.plot(kind='line', marker='o')\n",
        "            plt.title('Monthly Trends')\n",
        "            plt.savefig('visualizations/monthly_trends.png')\n",
        "            plt.close()\n",
        "\n",
        "class IncomePredictor:\n",
        "    def __init__(self, data, target_column='income'):\n",
        "        \"\"\"Initialize with data and target column name\"\"\"\n",
        "        self.data = data\n",
        "        self.target_column = target_column\n",
        "        self.models = {}\n",
        "        self.best_model = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare data for modeling\"\"\"\n",
        "        print(\"\\n=== Preparing Data for Modeling ===\")\n",
        "\n",
        "        # Separate features and target\n",
        "        X = self.data.drop(self.target_column, axis=1)\n",
        "        y = (self.data[self.target_column] == '>50K').astype(int)  # Convert to binary\n",
        "\n",
        "        # Identify numerical and categorical columns\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "        categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Create copies to avoid SettingWithCopyWarning\n",
        "        X_processed = X.copy()\n",
        "\n",
        "        # Scale numerical features\n",
        "        scaler = StandardScaler()\n",
        "        X_processed[numerical_features] = scaler.fit_transform(X_processed[numerical_features])\n",
        "\n",
        "        # Encode categorical features\n",
        "        for column in categorical_features:\n",
        "            # Create dummy variables\n",
        "            dummies = pd.get_dummies(X_processed[column], prefix=column, drop_first=True)\n",
        "            # Add dummy columns to processed features\n",
        "            X_processed = pd.concat([X_processed, dummies], axis=1)\n",
        "            # Drop original categorical column\n",
        "            X_processed.drop(column, axis=1, inplace=True)\n",
        "\n",
        "        # Split the data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X_processed, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(\"Data preparation completed\")\n",
        "        print(f\"Training set shape: {self.X_train.shape}\")\n",
        "        print(f\"Testing set shape: {self.X_test.shape}\")\n",
        "\n",
        "        # Store feature names for later use\n",
        "        self.feature_names = X_processed.columns.tolist()\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train multiple classification models\"\"\"\n",
        "        print(\"\\n=== Training Models ===\")\n",
        "\n",
        "        # Define models\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "            'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6,\n",
        "                random_state=42,\n",
        "                eval_metric='logloss'\n",
        "            ),\n",
        "            'SVM': SVC(probability=True, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each model\n",
        "        results = {}\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(self.X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                self.y_test, y_pred, average='binary'\n",
        "            )\n",
        "\n",
        "            results[name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "            print(f\"{name} Results:\")\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"Precision: {precision:.4f}\")\n",
        "            print(f\"Recall: {recall:.4f}\")\n",
        "            print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "            # Add feature importance for tree-based models\n",
        "            if name in ['Decision Tree', 'Random Forest', 'XGBoost']:\n",
        "                self._plot_feature_importance(model, name)\n",
        "\n",
        "        # Plot ROC curves\n",
        "        self._plot_roc_curves()\n",
        "\n",
        "        # Select best model\n",
        "        self.best_model = max(results.items(), key=lambda x: x[1]['f1'])[0]\n",
        "        print(f\"\\nBest performing model: {self.best_model}\")\n",
        "\n",
        "    def _plot_feature_importance(self, model, name):\n",
        "        \"\"\"Plot feature importance for tree-based models\"\"\"\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            indices = np.argsort(importance)[::-1]\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.title(f'Feature Importance ({name})')\n",
        "            plt.bar(range(len(importance)), importance[indices])\n",
        "            plt.xticks(range(len(importance)),\n",
        "                      [self.feature_names[i] for i in indices],\n",
        "                      rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'visualizations/feature_importance_{name.lower().replace(\" \", \"_\")}.png')\n",
        "            plt.close()\n",
        "\n",
        "    def _plot_roc_curves(self):\n",
        "        \"\"\"Plot ROC curves for all models\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            y_pred_proba = model.predict_proba(self.X_test)[:, 1]\n",
        "            fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves for Different Models')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig('visualizations/roc_curves.png')\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the analysis\"\"\"\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Initialize analyzers\n",
        "    data_analyzer = DataAnalyzer(data)\n",
        "\n",
        "    # Perform data quality analysis and cleaning\n",
        "    data_analyzer.analyze_data_quality()\n",
        "    data_analyzer.clean_data()\n",
        "\n",
        "    # Perform EDA\n",
        "    data_analyzer.perform_eda()\n",
        "\n",
        "    # Prepare for income prediction\n",
        "    income_predictor = IncomePredictor(data)\n",
        "    income_predictor.prepare_data()\n",
        "    income_predictor.train_models()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HjMsjJee_Srr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ca68d7-4f3c-4bfd-a0fc-d50ee5d00828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Data Quality Analysis ===\n",
            "\n",
            "Dataset Shape: (32561, 15)\n",
            "\n",
            "Data Types:\n",
            " age                int64\n",
            "workclass         object\n",
            "fnlwgt             int64\n",
            "education         object\n",
            "education-num      int64\n",
            "marital-status    object\n",
            "occupation        object\n",
            "relationship      object\n",
            "race              object\n",
            "sex               object\n",
            "capital-gain       int64\n",
            "capital-loss       int64\n",
            "hours-per-week     int64\n",
            "native-country    object\n",
            "income            object\n",
            "dtype: object\n",
            "\n",
            "Missing Values:\n",
            " age               0\n",
            "workclass         0\n",
            "fnlwgt            0\n",
            "education         0\n",
            "education-num     0\n",
            "marital-status    0\n",
            "occupation        0\n",
            "relationship      0\n",
            "race              0\n",
            "sex               0\n",
            "capital-gain      0\n",
            "capital-loss      0\n",
            "hours-per-week    0\n",
            "native-country    0\n",
            "income            0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate Rows: 24\n",
            "\n",
            "Memory Usage: 19.88 MB\n",
            "\n",
            "=== Data Cleaning ===\n",
            "Removed 24 duplicate rows\n",
            "Missing values handled\n",
            "\n",
            "=== Exploratory Data Analysis ===\n",
            "\n",
            "Numerical Features Summary:\n",
            "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
            "count  32537.000000  3.253700e+04   32537.000000  32537.000000  32537.000000   \n",
            "mean      38.585549  1.897808e+05      10.081815   1078.443741     87.368227   \n",
            "std       13.637984  1.055565e+05       2.571633   7387.957424    403.101833   \n",
            "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
            "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
            "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
            "75%       48.000000  2.369930e+05      12.000000      0.000000      0.000000   \n",
            "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
            "\n",
            "       hours-per-week  \n",
            "count    32537.000000  \n",
            "mean        40.440329  \n",
            "std         12.346889  \n",
            "min          1.000000  \n",
            "25%         40.000000  \n",
            "50%         40.000000  \n",
            "75%         45.000000  \n",
            "max         99.000000  \n",
            "\n",
            "Categorical Features Summary:\n",
            "\n",
            "workclass value counts:\n",
            "workclass\n",
            "Private             22673\n",
            "Self-emp-not-inc     2540\n",
            "Local-gov            2093\n",
            "?                    1836\n",
            "State-gov            1298\n",
            "Name: count, dtype: int64\n",
            "\n",
            "education value counts:\n",
            "education\n",
            "HS-grad         10494\n",
            "Some-college     7282\n",
            "Bachelors        5353\n",
            "Masters          1722\n",
            "Assoc-voc        1382\n",
            "Name: count, dtype: int64\n",
            "\n",
            "marital-status value counts:\n",
            "marital-status\n",
            "Married-civ-spouse    14970\n",
            "Never-married         10667\n",
            "Divorced               4441\n",
            "Separated              1025\n",
            "Widowed                 993\n",
            "Name: count, dtype: int64\n",
            "\n",
            "occupation value counts:\n",
            "occupation\n",
            "Prof-specialty     4136\n",
            "Craft-repair       4094\n",
            "Exec-managerial    4065\n",
            "Adm-clerical       3768\n",
            "Sales              3650\n",
            "Name: count, dtype: int64\n",
            "\n",
            "relationship value counts:\n",
            "relationship\n",
            "Husband          13187\n",
            "Not-in-family     8292\n",
            "Own-child         5064\n",
            "Unmarried         3445\n",
            "Wife              1568\n",
            "Name: count, dtype: int64\n",
            "\n",
            "race value counts:\n",
            "race\n",
            "White                 27795\n",
            "Black                  3122\n",
            "Asian-Pac-Islander     1038\n",
            "Amer-Indian-Eskimo      311\n",
            "Other                   271\n",
            "Name: count, dtype: int64\n",
            "\n",
            "sex value counts:\n",
            "sex\n",
            "Male      21775\n",
            "Female    10762\n",
            "Name: count, dtype: int64\n",
            "\n",
            "native-country value counts:\n",
            "native-country\n",
            "United-States    29153\n",
            "Mexico             639\n",
            "?                  582\n",
            "Philippines        198\n",
            "Germany            137\n",
            "Name: count, dtype: int64\n",
            "\n",
            "income value counts:\n",
            "income\n",
            "<=50K    24698\n",
            ">50K      7839\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Preparing Data for Modeling ===\n",
            "Data preparation completed\n",
            "Training set shape: (26029, 100)\n",
            "Testing set shape: (6508, 100)\n",
            "\n",
            "=== Training Models ===\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.8568\n",
            "Precision: 0.7494\n",
            "Recall: 0.6288\n",
            "F1-score: 0.6839\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "Accuracy: 0.8168\n",
            "Precision: 0.6243\n",
            "Recall: 0.6438\n",
            "F1-score: 0.6339\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "Accuracy: 0.8588\n",
            "Precision: 0.7526\n",
            "Recall: 0.6357\n",
            "F1-score: 0.6892\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost Results:\n",
            "Accuracy: 0.8722\n",
            "Precision: 0.7892\n",
            "Recall: 0.6563\n",
            "F1-score: 0.7166\n",
            "\n",
            "Training SVM...\n",
            "SVM Results:\n",
            "Accuracy: 0.8554\n",
            "Precision: 0.7635\n",
            "Recall: 0.5983\n",
            "F1-score: 0.6709\n",
            "\n",
            "Best performing model: XGBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mQI2LrU3p62O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Results and Inferences\n",
        "\n",
        "**Guidelines**: List out your inferences here"
      ],
      "metadata": {
        "id": "QxRUBWFt_VH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Observation 1: Average age is around 38\n",
        "Observation 2: Most frequent education level is High School Graduate\n",
        "Observation 3: The dataset is imbalanced which could lead to some misleading outputs\n",
        "Observation 4: All models have around 85% accuracy which is pretty good\n",
        "Observation 5: XGBoost is the best performing model\n",
        "\n",
        "Note: If we had to fix the dataset imbalance we could oversample the minority class using smote \"\"\""
      ],
      "metadata": {
        "id": "Tvbq02Qe_lh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}